{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_profiling\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite, ExpectationConfiguration\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "import streamlit as st\n",
    "\n",
    "def profile_data_with_pandas_profiling(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Pandas-Profiling.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # 2. Generate a profile report with default settings\n",
    "        profile = pandas_profiling.ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "\n",
    "        # Save the report\n",
    "        profile_file_name = file_path.replace(\".csv\", \"_profile.html\")\n",
    "        profile.to_file(profile_file_name)\n",
    "        st.success(f\"Pandas-Profiling report generated and saved to: {profile_file_name}\")\n",
    "\n",
    "        # Display the report in Streamlit\n",
    "        st.header(\"Pandas Profiling Report\")\n",
    "        st.components.v1.html(profile.to_html(), width=800, height=600, scrolling=True)\n",
    "\n",
    "        # 3. Customize the profile report to include correlations and a subset of columns\n",
    "        # Calculate correlations (e.g., Pearson correlation)\n",
    "        profile_with_corr = pandas_profiling.ProfileReport(\n",
    "            df,\n",
    "            title=\"Pandas Profiling Report with Correlations\",\n",
    "            explorative=True,  # Enable more detailed exploration\n",
    "            correlations={\"pearson\": {\"calculate\": True}},  # Calculate Pearson correlation\n",
    "        )\n",
    "\n",
    "        # Save the report with correlations\n",
    "        profile_with_corr_filename = file_path.replace(\".csv\", \"_profile_with_corr.html\")\n",
    "        profile_with_corr.to_file(profile_with_corr_filename)\n",
    "        st.success(f\"Pandas-Profiling report with correlations generated and saved to: {profile_with_corr_filename}\")\n",
    "\n",
    "        # Profile a subset of columns\n",
    "        if 'age' in df.columns and 'income' in df.columns:  # Check if the columns exist\n",
    "            subset_profile = pandas_profiling.ProfileReport(\n",
    "                df[['age', 'income']],  # Profile only 'age' and 'income'\n",
    "                title=\"Pandas Profiling Report - Subset of Columns\",\n",
    "            )\n",
    "            subset_profile_filename = file_path.replace(\".csv\", \"_profile_subset.html\")\n",
    "            subset_profile.to_file(subset_profile_filename)\n",
    "            st.success(f\"Pandas-Profiling report for subset of columns generated and saved to: {subset_profile_filename}\")\n",
    "        else:\n",
    "            st.warning(\"Columns 'age' and 'income' not found in the DataFrame.  Skipping subset profiling.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def profile_data_with_great_expectations(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Great Expectations.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # Create a Great Expectations context\n",
    "        context = ge.get_context()\n",
    "\n",
    "        # Create an expectation suite (or load an existing one)\n",
    "        expectation_suite_name = \"my_data_suite\"  # You can choose any name\n",
    "        suite = context.get_expectation_suite(expectation_suite_name)\n",
    "        if suite is None:\n",
    "            suite = context.create_expectation_suite(expectation_suite_name)\n",
    "\n",
    "        # Add multiple expectations to the suite\n",
    "        suite.add_expectation(\n",
    "            ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                expectation_type=\"expect_table_columns_to_exist\",\n",
    "                kwargs={\"column_names\": df.columns.tolist()},\n",
    "            )\n",
    "        )\n",
    "        if 'age' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_be_between\",\n",
    "                    kwargs={\n",
    "                        \"column\": \"age\",\n",
    "                        \"min_value\": 0,\n",
    "                        \"max_value\": 150,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        if 'income' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "                    kwargs={\"column\": \"income\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        context.save_expectation_suite(suite, expectation_suite_name=expectation_suite_name) #save the suite\n",
    "\n",
    "        # Create a BatchRequest\n",
    "        batch_request = ge.datasource.Datasource.create_batch_request(\n",
    "            datasource_name=\"pandas\",  # Use the name 'pandas'\n",
    "            data_asset_name=\"my_data\",  # You can choose any name\n",
    "            pandas_df=df,  # Pass the DataFrame here\n",
    "        )\n",
    "\n",
    "        # 4. Validate data against the expectation suite\n",
    "        validator = context.get_validator(\n",
    "            batch_request=batch_request,\n",
    "            expectation_suite_name=expectation_suite_name,\n",
    "        )\n",
    "        validation_results = validator.validate()\n",
    "\n",
    "        # Print the validation results\n",
    "        st.header(\"Great Expectations Validation Results\")\n",
    "        st.json(validation_results)\n",
    "\n",
    "        # Check if the validation was successful\n",
    "        if validation_results[\"success\"]:\n",
    "            st.success(\"Data validation with Great Expectations was successful!\")\n",
    "        else:\n",
    "            st.error(\"Data validation with Great Expectations failed.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the file path from the user using st.text_input\n",
    "    default_file_path = \"data.csv\"  # Replace with your default CSV file name\n",
    "    file_path = st.text_input(\"Enter the path to your CSV file:\", default_file_path)\n",
    "\n",
    "    # Profile data with Pandas-Profiling\n",
    "    st.header(\"Data Profiling with Pandas-Profiling\")\n",
    "    profile_data_with_pandas_profiling(file_path)\n",
    "\n",
    "    # Profile data with Great Expectations\n",
    "    st.header(\"Data Profiling with Great Expectations\")\n",
    "    profile_data_with_great_expectations(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_profiling\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite, ExpectationConfiguration\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "import streamlit as st\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import datetime\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level (e.g., INFO, WARNING, ERROR)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],  # Output to the console\n",
    ")\n",
    "logger = logging.getLogger(__name__)  # Get the logger for this module\n",
    "\n",
    "\n",
    "def profile_data_with_pandas_profiling(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Pandas-Profiling.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # 2. Generate a profile report with default settings\n",
    "        profile = pandas_profiling.ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "\n",
    "        # Save the report\n",
    "        profile_file_name = file_path.replace(\".csv\", \"_profile.html\")\n",
    "        profile.to_file(profile_file_name)\n",
    "        st.success(f\"Pandas-Profiling report generated and saved to: {profile_file_name}\")\n",
    "\n",
    "        # Display the report in Streamlit\n",
    "        st.header(\"Pandas Profiling Report\")\n",
    "        st.components.v1.html(profile.to_html(), width=800, height=600, scrolling=True)\n",
    "\n",
    "        # 3. Customize the profile report to include correlations and a subset of columns\n",
    "        # Calculate correlations (e.g., Pearson correlation)\n",
    "        profile_with_corr = pandas_profiling.ProfileReport(\n",
    "            df,\n",
    "            title=\"Pandas Profiling Report with Correlations\",\n",
    "            explorative=True,  # Enable more detailed exploration\n",
    "            correlations={\"pearson\": {\"calculate\": True}},  # Calculate Pearson correlation\n",
    "        )\n",
    "\n",
    "        # Save the report with correlations\n",
    "        profile_with_corr_filename = file_path.replace(\".csv\", \"_profile_with_corr.html\")\n",
    "        profile_with_corr.to_file(profile_with_corr_filename)\n",
    "        st.success(f\"Pandas-Profiling report with correlations generated and saved to: {profile_with_corr_filename}\")\n",
    "\n",
    "        # Profile a subset of columns\n",
    "        if 'age' in df.columns and 'income' in df.columns:  # Check if the columns exist\n",
    "            subset_profile = pandas_profiling.ProfileReport(\n",
    "                df[['age', 'income']],  # Profile only 'age' and 'income'\n",
    "                title=\"Pandas Profiling Report - Subset of Columns\",\n",
    "            )\n",
    "            subset_profile_filename = file_path.replace(\".csv\", \"_profile_subset.html\")\n",
    "            subset_profile.to_file(subset_profile_filename)\n",
    "            st.success(f\"Pandas-Profiling report for subset of columns generated and saved to: {subset_profile_filename}\")\n",
    "        else:\n",
    "            st.warning(\"Columns 'age' and 'income' not found in the DataFrame.  Skipping subset profiling.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def profile_data_with_great_expectations(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Great Expectations.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # Create a Great Expectations context\n",
    "        context = ge.get_context()\n",
    "\n",
    "        # Create an expectation suite (or load an existing one)\n",
    "        expectation_suite_name = \"my_data_suite\"  # You can choose any name\n",
    "        suite = context.get_expectation_suite(expectation_suite_name)\n",
    "        if suite is None:\n",
    "            suite = context.create_expectation_suite(expectation_suite_name)\n",
    "\n",
    "        # Add multiple expectations to the suite\n",
    "        suite.add_expectation(\n",
    "            ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                expectation_type=\"expect_table_columns_to_exist\",\n",
    "                kwargs={\"column_names\": df.columns.tolist()},\n",
    "            )\n",
    "        )\n",
    "        if 'age' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_be_between\",\n",
    "                    kwargs={\n",
    "                        \"column\": \"age\",\n",
    "                        \"min_value\": 0,\n",
    "                        \"max_value\": 150,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        if 'income' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "                    kwargs={\"column\": \"income\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        context.save_expectation_suite(suite, expectation_suite_name=expectation_suite_name) #save the suite\n",
    "\n",
    "        # Create a BatchRequest\n",
    "        batch_request = ge.datasource.Datasource.create_batch_request(\n",
    "            datasource_name=\"pandas\",  # Use the name 'pandas'\n",
    "            data_asset_name=\"my_data\",  # You can choose any name\n",
    "            pandas_df=df,  # Pass the DataFrame here\n",
    "        )\n",
    "\n",
    "        # 4. Validate data against the expectation suite\n",
    "        validator = context.get_validator(\n",
    "            batch_request=batch_request,\n",
    "            expectation_suite_name=expectation_suite_name,\n",
    "        )\n",
    "        validation_results = validator.validate()\n",
    "\n",
    "        # Print the validation results\n",
    "        st.header(\"Great Expectations Validation Results\")\n",
    "        st.json(validation_results)\n",
    "\n",
    "        # 5. Check if the validation was successful\n",
    "        if validation_results[\"success\"]:\n",
    "            st.success(\"Data validation with Great Expectations was successful!\")\n",
    "            return True  # Return True for success\n",
    "        else:\n",
    "            st.error(\"Data validation with Great Expectations failed!\")\n",
    "            return False  # Return False for failure\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def send_email_alert(subject, body):\n",
    "    \"\"\"\n",
    "    Sends an email alert.\n",
    "\n",
    "    Args:\n",
    "        subject (str): The subject of the email.\n",
    "        body (str): The body of the email.\n",
    "    \"\"\"\n",
    "    sender_email = \"your_email@example.com\"  # Replace with your email address\n",
    "    receiver_email = \"recipient_email@example.com\"  # Replace with the recipient's email address\n",
    "    smtp_server = \"smtp.example.com\"  # Replace with your SMTP server address\n",
    "    smtp_port = 587  # Replace with your SMTP server port (e.g., 587 for TLS)\n",
    "    smtp_username = \"your_email@example.com\"  # Replace with your email username\n",
    "    smtp_password = \"your_email_password\"  # Replace with your email password or an app password\n",
    "\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = receiver_email\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "        server.starttls()\n",
    "        server.login(smtp_username, smtp_password)\n",
    "        server.sendmail(sender_email, [receiver_email], msg.as_string())\n",
    "        server.quit()\n",
    "        logger.info(\"Email alert sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error sending email alert: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def monitor_data_quality(file_path):\n",
    "    \"\"\"\n",
    "    Monitors data quality in real-time (simulated) by repeatedly profiling\n",
    "    data with Great Expectations and sending alerts if quality drops.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Simulate data updates (replace with your actual data source)\n",
    "        try:\n",
    "            # In a real-world scenario, you would read data from a stream,\n",
    "            # a database, or an API.  Here, we simulate reading from a file\n",
    "            # that might be updated periodically.\n",
    "            df = pd.read_csv(file_path) #read the file\n",
    "            # Introduce some random changes to simulate data changes\n",
    "            if random.random() < 0.2:  # 20% chance of introducing a change\n",
    "                if 'age' in df.columns:\n",
    "                    df['age'] = df['age'].apply(lambda x: x + random.randint(-5, 5))  # Change age\n",
    "                if 'income' in df.columns:\n",
    "                    df['income'] = df['income'].apply(lambda x: x * (1 + random.uniform(-0.1, 0.1)) if pd.notnull(x) else None) #change income\n",
    "\n",
    "            df.to_csv(file_path, index=False)  # Save the modified DataFrame back to the CSV file\n",
    "            logger.info(f\"Simulated data update at {datetime.datetime.now()}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading or updating data: {e}\")\n",
    "\n",
    "        # Profile data with Great Expectations\n",
    "        validation_status = profile_data_with_great_expectations(file_path)\n",
    "\n",
    "        if not validation_status:\n",
    "            # 1. Set up a basic alert using the logging library\n",
    "            logger.error(\"Data quality check failed!\")\n",
    "\n",
    "            # 2. Implement alerts using email notifications\n",
    "            subject = \"Data Quality Alert\"\n",
    "            body = f\"Data quality check failed at {datetime.datetime.now()}.  Please investigate the data in {file_path}.\"\n",
    "            send_email_alert(subject, body)\n",
    "\n",
    "            # 3.  Integrate with a dashboard like Grafana for visual alerts\n",
    "            #    (Conceptual - requires a separate Grafana setup)\n",
    "            #    In a real application, you would send metrics to Grafana\n",
    "            #    (e.g., number of failed expectations) and configure\n",
    "            #    alerts in Grafana based on those metrics.\n",
    "            #    This is a placeholder for that functionality.\n",
    "            logger.info(\n",
    "                \"Sending metrics to Grafana (simulated).  \"\n",
    "                \"In a real setup, configure Grafana to alert on these metrics.\"\n",
    "            )\n",
    "            #  Example Grafana metric (conceptual):\n",
    "            #  metric_name: data_quality_check_failed\n",
    "            #  value: 1 (if failed), 0 (if passed)\n",
    "            #  In a real implementation, you'd use a Grafana client library\n",
    "            #  to send this metric.\n",
    "\n",
    "        time.sleep(60)  # Check every 60 seconds (adjust as needed)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the file path from the user using st.text_input\n",
    "    default_file_path = \"data.csv\"  # Replace with your default CSV file name\n",
    "    file_path = st.text_input(\"Enter the path to your CSV file:\", default_file_path)\n",
    "\n",
    "    # Create a sample CSV file if it doesn't exist\n",
    "    if not pd.io.common.file_exists(file_path):\n",
    "        df = pd.DataFrame({\n",
    "            'age': [20, 30, 40, 50, 60],\n",
    "            'income': [50000, 60000, 70000, 80000, 90000]\n",
    "        })\n",
    "        df.to_csv(file_path, index=False)\n",
    "        st.info(f\"Created a sample CSV file at {file_path}.  You can replace this with your own data.\")\n",
    "\n",
    "    # Profile data with Pandas-Profiling\n",
    "    st.header(\"Data Profiling with Pandas-Profiling\")\n",
    "    profile_data_with_pandas_profiling(file_path)\n",
    "\n",
    "    # Profile data with Great Expectations\n",
    "    st.header(\"Data Profiling with Great Expectations\")\n",
    "    profile_data_with_great_expectations(file_path)\n",
    "\n",
    "    # Start real-time monitoring in a separate thread\n",
    "    monitoring_thread = threading.Thread(target=monitor_data_quality, args=(file_path,))\n",
    "    monitoring_thread.daemon = True  # Allow the main thread to exit\n",
    "    monitoring_thread.start()\n",
    "\n",
    "    st.subheader(\"Real-time Data Quality Monitoring\")\n",
    "    st.write(\"Data quality is being monitored in the background.  \"\n",
    "             \"Check the console for log messages and email alerts (if any issues are detected).\")\n",
    "\n",
    "    # Keep the main thread alive.\n",
    "    while True:\n",
    "        time.sleep(600)  # Check every 10 minutes\n",
    "        st.info(\"The main thread is still alive. Monitoring is running in the background...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas_profiling\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite, ExpectationConfiguration\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "import streamlit as st\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import datetime\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level (e.g., INFO, WARNING, ERROR)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],  # Output to the console\n",
    ")\n",
    "logger = logging.getLogger(__name__)  # Get the logger for this module\n",
    "\n",
    "\n",
    "def profile_data_with_pandas_profiling(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Pandas-Profiling.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # 2. Generate a profile report with default settings\n",
    "        profile = pandas_profiling.ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "\n",
    "        # Save the report\n",
    "        profile_file_name = file_path.replace(\".csv\", \"_profile.html\")\n",
    "        profile.to_file(profile_file_name)\n",
    "        st.success(f\"Pandas-Profiling report generated and saved to: {profile_file_name}\")\n",
    "\n",
    "        # Display the report in Streamlit\n",
    "        st.header(\"Pandas Profiling Report\")\n",
    "        st.components.v1.html(profile.to_html(), width=800, height=600, scrolling=True)\n",
    "\n",
    "        # 3. Customize the profile report to include correlations and a subset of columns\n",
    "        # Calculate correlations (e.g., Pearson correlation)\n",
    "        profile_with_corr = pandas_profiling.ProfileReport(\n",
    "            df,\n",
    "            title=\"Pandas Profiling Report with Correlations\",\n",
    "            explorative=True,  # Enable more detailed exploration\n",
    "            correlations={\"pearson\": {\"calculate\": True}},  # Calculate Pearson correlation\n",
    "        )\n",
    "\n",
    "        # Save the report with correlations\n",
    "        profile_with_corr_filename = file_path.replace(\".csv\", \"_profile_with_corr.html\")\n",
    "        profile_with_corr.to_file(profile_with_corr_filename)\n",
    "        st.success(f\"Pandas-Profiling report with correlations generated and saved to: {profile_with_corr_filename}\")\n",
    "\n",
    "        # Profile a subset of columns\n",
    "        if 'age' in df.columns and 'income' in df.columns:  # Check if the columns exist\n",
    "            subset_profile = pandas_profiling.ProfileReport(\n",
    "                df[['age', 'income']],  # Profile only 'age' and 'income'\n",
    "                title=\"Pandas Profiling Report - Subset of Columns\",\n",
    "            )\n",
    "            subset_profile_filename = file_path.replace(\".csv\", \"_profile_subset.html\")\n",
    "            subset_profile.to_file(subset_profile_filename)\n",
    "            st.success(f\"Pandas-Profiling report for subset of columns generated and saved to: {subset_profile_filename}\")\n",
    "        else:\n",
    "            st.warning(\"Columns 'age' and 'income' not found in the DataFrame.  Skipping subset profiling.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def profile_data_with_great_expectations(file_path):\n",
    "    \"\"\"\n",
    "    Profiles data from a CSV file using Great Expectations.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file into a Pandas DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        st.write(f\"Loaded data from: {file_path}\")\n",
    "\n",
    "        # Create a Great Expectations context\n",
    "        context = ge.get_context()\n",
    "\n",
    "        # Create an expectation suite (or load an existing one)\n",
    "        expectation_suite_name = \"my_data_suite\"  # You can choose any name\n",
    "        suite = context.get_expectation_suite(expectation_suite_name)\n",
    "        if suite is None:\n",
    "            suite = context.create_expectation_suite(expectation_suite_name)\n",
    "\n",
    "        # Add multiple expectations to the suite\n",
    "        suite.add_expectation(\n",
    "            ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                expectation_type=\"expect_table_columns_to_exist\",\n",
    "                kwargs={\"column_names\": df.columns.tolist()},\n",
    "            )\n",
    "        )\n",
    "        if 'age' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_be_between\",\n",
    "                    kwargs={\n",
    "                        \"column\": \"age\",\n",
    "                        \"min_value\": 0,\n",
    "                        \"max_value\": 150,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        if 'income' in df.columns:\n",
    "            suite.add_expectation(\n",
    "                ge.core.ExpectationConfiguration(  # Use ge.core.ExpectationConfiguration\n",
    "                    expectation_type=\"expect_column_values_to_not_be_null\",\n",
    "                    kwargs={\"column\": \"income\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        context.save_expectation_suite(suite, expectation_suite_name=expectation_suite_name) #save the suite\n",
    "\n",
    "        # Create a BatchRequest\n",
    "        batch_request = ge.datasource.Datasource.create_batch_request(\n",
    "            datasource_name=\"pandas\",  # Use the name 'pandas'\n",
    "            data_asset_name=\"my_data\",  # You can choose any name\n",
    "            pandas_df=df,  # Pass the DataFrame here\n",
    "        )\n",
    "\n",
    "        # 4. Validate data against the expectation suite\n",
    "        validator = context.get_validator(\n",
    "            batch_request=batch_request,\n",
    "            expectation_suite_name=expectation_suite_name,\n",
    "        )\n",
    "        validation_results = validator.validate()\n",
    "\n",
    "        # Print the validation results\n",
    "        st.header(\"Great Expectations Validation Results\")\n",
    "        st.json(validation_results)\n",
    "\n",
    "        # 5. Check if the validation was successful\n",
    "        if validation_results[\"success\"]:\n",
    "            st.success(\"Data validation with Great Expectations was successful!\")\n",
    "            return True  # Return True for success\n",
    "        else:\n",
    "            st.error(\"Data validation with Great Expectations failed!\")\n",
    "            return False  # Return False for failure\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def send_email_alert(subject, body):\n",
    "    \"\"\"\n",
    "    Sends an email alert.\n",
    "\n",
    "    Args:\n",
    "        subject (str): The subject of the email.\n",
    "        body (str): The body of the email.\n",
    "    \"\"\"\n",
    "    sender_email = \"your_email@example.com\"  # Replace with your email address\n",
    "    receiver_email = \"recipient_email@example.com\"  # Replace with the recipient's email address\n",
    "    smtp_server = \"smtp.example.com\"  # Replace with your SMTP server address\n",
    "    smtp_port = 587  # Replace with your SMTP server port (e.g., 587 for TLS)\n",
    "    smtp_username = \"your_email@example.com\"  # Replace with your email username\n",
    "    smtp_password = \"your_email_password\"  # Replace with your email password or an app password\n",
    "\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = receiver_email\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "        server.starttls()\n",
    "        server.login(smtp_username, smtp_password)\n",
    "        server.sendmail(sender_email, [receiver_email], msg.as_string())\n",
    "        server.quit()\n",
    "        logger.info(\"Email alert sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error sending email alert: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def detect_anomalies(df):\n",
    "    \"\"\"\n",
    "    Detects anomalies in the given DataFrame using Isolation Forest.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the detected anomalies, or an empty DataFrame if no anomalies are found.\n",
    "    \"\"\"\n",
    "    # Handle missing values using imputation (replace NaN with the mean of the column)\n",
    "    df_imputed = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "    for col in df_imputed.columns:\n",
    "        if df_imputed[col].isnull().any():\n",
    "            mean_val = df_imputed[col].mean()\n",
    "            df_imputed[col] = df_imputed[col].fillna(mean_val)\n",
    "\n",
    "    # Initialize and fit the Isolation Forest model\n",
    "    model = IsolationForest(contamination='auto', random_state=42)  # Set random_state for reproducibility\n",
    "    model.fit(df_imputed)\n",
    "\n",
    "    # Predict anomalies (returns 1 for inliers, -1 for outliers)\n",
    "    anomaly_labels = model.predict(df_imputed)\n",
    "\n",
    "    # Get the anomaly data points\n",
    "    anomaly_data = df[anomaly_labels == -1]\n",
    "    return anomaly_data\n",
    "\n",
    "\n",
    "\n",
    "def monitor_data_quality(file_path):\n",
    "    \"\"\"\n",
    "    Monitors data quality in real-time (simulated) by repeatedly profiling\n",
    "    data with Great Expectations and using AI (Isolation Forest) to detect anomalies.\n",
    "    Sends alerts if quality drops or anomalies are detected.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Simulate data updates\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Introduce random changes\n",
    "            if random.random() < 0.2:\n",
    "                if 'age' in df.columns:\n",
    "                    df['age'] = df['age'].apply(lambda x: x + random.randint(-5, 5))\n",
    "                if 'income' in df.columns:\n",
    "                    df['income'] = df['income'].apply(lambda x: x * (1 + random.uniform(-0.1, 0.1)) if pd.notnull(x) else None)\n",
    "            df.to_csv(file_path, index=False)\n",
    "            logger.info(f\"Simulated data update at {datetime.datetime.now()}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading or updating data: {e}\")\n",
    "            continue  # Go to the next iteration of the loop\n",
    "\n",
    "        # 1. Basic AI Models for Monitoring\n",
    "        #   - Train a simple anomaly detection model using Isolation Forest.\n",
    "        anomaly_data = detect_anomalies(df)\n",
    "\n",
    "        # 2. Use a simple custom function based AI logic for outlier detection.\n",
    "        #    (Example: check if age is outside of expected range)\n",
    "        anomalous_age_data = df[(df['age'] < 0) | (df['age'] > 120)] #0-120\n",
    "        if not anomalous_age_data.empty:\n",
    "            logger.warning(f\"Outliers detected in 'age' column: {anomalous_age_data.to_string()}\")\n",
    "\n",
    "        # Profile data with Great Expectations\n",
    "        validation_status = profile_data_with_great_expectations(file_path)\n",
    "\n",
    "        # 3. Creating a monitoring function that utilizes a pre-trained machine learning model.\n",
    "        #    (This is already happening with the use of detect_anomalies)\n",
    "        if not validation_status or not anomaly_data.empty or not anomalous_age_data.empty: #send if GE validation fails or anomalies detected\n",
    "            subject = \"Data Quality and Anomaly Alert\"\n",
    "            body = f\"Data quality check failed at {datetime.datetime.now()}.  Please investigate the data in {file_path}.\"\n",
    "            if not validation_status:\n",
    "                body += \"\\n\\nGreat Expectations validation failed.\"\n",
    "            if not anomaly_data.empty:\n",
    "                body += \"\\n\\nAnomalies detected by Isolation Forest:\\n\"\n",
    "                body += anomaly_data.to_string()\n",
    "            if not anomalous_age_data.empty:\n",
    "                body += \"\\n\\nAnomalies detected in age column:\\n\"\n",
    "                body += anomalous_age_data.to_string()\n",
    "            send_email_alert(subject, body)\n",
    "            logger.error(f\"Data quality check failed!\")\n",
    "        else:\n",
    "            logger.info(\"Data quality check successful.\")\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the file path from the user using st.text_input\n",
    "    default_file_path = \"data.csv\"  # Replace with your default CSV file name\n",
    "    file_path = st.text_input(\"Enter the path to your CSV file:\", default_file_path)\n",
    "\n",
    "    # Create a sample CSV file if it doesn't exist\n",
    "    if not pd.io.common.file_exists(file_path):\n",
    "        df = pd.DataFrame({\n",
    "            'age': [20, 30, 40, 50, 60],\n",
    "            'income': [50000, 60000, 70000, 80000, 90000]\n",
    "        })\n",
    "        df.to_csv(file_path, index=False)\n",
    "        st.info(f\"Created a sample CSV file at {file_path}.  You can replace this with your own data.\")\n",
    "\n",
    "    # Profile data with Pandas-Profiling\n",
    "    st.header(\"Data Profiling with Pandas-Profiling\")\n",
    "    profile_data_with_pandas_profiling(file_path)\n",
    "\n",
    "    # Profile data with Great Expectations\n",
    "    st.header(\"Data Profiling with Great Expectations\")\n",
    "    profile_data_with_great_expectations(file_path)\n",
    "\n",
    "    # Start real-time monitoring in a separate thread\n",
    "    monitoring_thread = threading.Thread(target=monitor_data_quality, args=(file_path,))\n",
    "    monitoring_thread.daemon = True  # Allow the main thread to exit\n",
    "    monitoring_thread.start()\n",
    "\n",
    "    st.subheader(\"Real-time Data Quality Monitoring\")\n",
    "    st.write(\"Data quality is being monitored in the background.  \"\n",
    "             \"Check the console for log messages and email alerts (if any issues are detected).\")\n",
    "\n",
    "    # Keep the main thread alive.\n",
    "    while True:\n",
    "        time.sleep(600)\n",
    "        st.info(\"The main thread is still alive. Monitoring is running in the background...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
